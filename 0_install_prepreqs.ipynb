{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "**Tested on Ubuntu 22.04**\n",
    "Using vscode and remote ssh extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "## Install Linux \n",
    "\n",
    "[Ollama Linux installation instructions](https://ollama.com/download/linux)\n",
    "\n",
    "```sh\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "## Try it\n",
    "\n",
    "```sh\n",
    "ollama list\n",
    "ollama pull llama3.1:70b\n",
    "ollama run llama3.1:70b\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your virtualenv with conda\n",
    "\n",
    "```sh\n",
    "conda create -n plotomatic python=3.12\n",
    "conda activate plotomatic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove conda packages (if there are issues)\n",
    "If you see this error, there is a bug with nvidia and conda where you will probably need to remove al of the conda packages using `conda remove` with the same args to to uninstall them, then reinstall them after. I have seen this if I try to install a new VSCode extension, too.\n",
    "\n",
    "```InvalidSpec: The package \"nvidia/linux-64::cuda-compiler==12.6.2=0\" is not available for the specified platform```\n",
    "\n",
    "```sh\n",
    "# One of these breaks conda for new install (probably cuda), so remove them first before adding new packages\n",
    "conda remove \\\n",
    "        cuda cuda-nvcc cuda-cudart cuda-compiler \\\n",
    "        pytorch-cuda pytorch torchvision \\\n",
    "        tensorflow-gpu tensorflow cudnn \\\n",
    "        ipykernel sqlite nbconvert\n",
    "```\n",
    "\n",
    "## Install conda packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipykernel is for vscode\n",
    "# %conda install \\\n",
    "#         cuda cuda-nvcc cuda-cudart cuda-compiler \\\n",
    "#         pytorch-cuda pytorch torchvision \\\n",
    "#         tensorflow-gpu tensorflow cudnn \\\n",
    "#         ipykernel sqlite nbconvert\n",
    "\n",
    "# Restart the Juptyer Notebook kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuda, TensorRT\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html\n",
    "\n",
    "https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#install\n",
    "\n",
    "Skip the following if you aren't using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if nvidia drivers and cuda are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Version: 12.4\n",
    "# Driver Version: 550.107.02\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install NVIDIA drivers\n",
    "\n",
    "Only do this if `nvidia-smi` didn't work.\n",
    "\n",
    "```sh\n",
    "sudo ubuntu-drivers devices | grep recommended\n",
    "sudo apt-get install nvidia-driver-550\n",
    "sudo reboot\n",
    "```\n",
    "\n",
    "### Check the driver is installed and working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt list --installed | grep nvidia-driver\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install CUDA\n",
    "```sh\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install cuda-toolkit-12-4\n",
    "sudo apt-get install -y cuda-drivers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure TensorRT is installed\n",
    "\n",
    "!dpkg-query -W tensorrt \n",
    "# 10.5.0.18-1+cuda12.6\n",
    "\n",
    "!dpkg-query -W cuda-toolkit\n",
    "# 12.6.1-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip wheel setuptools \n",
    "\n",
    "# Ninja will help some packages compile faster\n",
    "%pip install ninja \n",
    "\n",
    "# These need to be installed first\n",
    "%pip install \\\n",
    "    nvidia-tensorrt \\\n",
    "    tensorflow \\\n",
    "    torch \\\n",
    "    tensorflow>=2.17.0 \\\n",
    "    cuda-python>=12.6.0 \\\n",
    "    torchvision>=0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure they work\n",
    "import tensorrt\n",
    "import tensorflow\n",
    "import cuda\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    huggingface_hub \\\n",
    "    transformers>=4.25.1 \\\n",
    "    diffusers \\\n",
    "    accelerate \\\n",
    "    ipywidgets \\\n",
    "    matplotlib \\\n",
    "    sentencepiece \\\n",
    "    numpy \\\n",
    "    rembg[GPU] \\\n",
    "    pydantic \\\n",
    "    unidecode \\\n",
    "    deepdiff \\\n",
    "    json-repair \\\n",
    "    ollama \\\n",
    "    graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamaindex\n",
    "%pip install llama-index-llms-ollama llama_index llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA NeMo Guardrails\n",
    "%pip install nemoguardrails llama-index-output-parsers-guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Cogvideo\n",
    "# TODO: Move this to a separate notebook\n",
    "# https://huggingface.co/THUDM/CogVideoX-5b-I2V\n",
    "# %pip install --upgrade transformers accelerate diffusers imageio-ffmpeg tbb xfuser[flash_attn] onediff\n",
    "\n",
    "# xfuser is for https://github.com/xdit-project/xDiT to run CogVideoX with parallel inference\n",
    "# onediff, nexfort is for single gpu acceleration with xdit\n",
    "\n",
    "# Acceleration for Cogvideo # full options are cpu/cu118/cu121/cu124\n",
    "# %pip install --pre torchao --index-url https://download.pytorch.org/whl/nightly/cu124 \n",
    "# %pip install optimum-quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster image processing\n",
    "# %pip uninstall pillow\n",
    "%pip install pillow-simd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For llama3-vision local\n",
    "# %pip install --upgrade transformers>=4.45.0\n",
    "\n",
    "# Transformers conflict with coquitts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how much memory is available\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CPU information\n",
    "!lscpu | head -n 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure GPU works from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def show_memory_usage():\n",
    "    # GPU Info\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_table = \"\"\"### GPU VRAM Usage\n",
    "\n",
    "| GPU Index | GPU Name | Used VRAM (GB) | Total VRAM (GB) | VRAM Usage (%) |\n",
    "|-----------|----------|------------------|-------------------|------------------|\n",
    "\"\"\"\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            free, total = torch.cuda.mem_get_info(i)\n",
    "            used = total - free\n",
    "            used_gb = used / 1024 ** 3\n",
    "            total_gb = total / 1024 ** 3\n",
    "            percent_used = used_gb / total_gb * 100.0\n",
    "\n",
    "            gpu_table += f\"| {i} | {gpu_name} | {used_gb:.2f} GB | {total_gb:.2f} GB | {percent_used:.2f}%        |\\n\"\n",
    "\n",
    "        display(Markdown(gpu_table))\n",
    "    else:\n",
    "        display(Markdown(\"**No GPU available**\"))\n",
    "\n",
    "\n",
    "show_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log into Hugging Face\n",
    "\n",
    "So we can pull models!\n",
    "\n",
    "https://huggingface.co/settings/tokens to get a token and set it in `HF_TOKEN` environment variable or do the following in your shell\n",
    "\n",
    "```sh\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "See https://huggingface.co/docs/huggingface_hub/en/quick-start for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure it works by fetching info about a model\n",
    "from huggingface_hub import ModelCard\n",
    "\n",
    "model_card = ModelCard.load('black-forest-labs/FLUX.1-dev')\n",
    "print(model_card.data.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash Attention\n",
    "\n",
    "Ninja will make it build much faster.\n",
    "\n",
    "Per [PyPi](https://pypi.org/project/flash-attn/):\n",
    "\n",
    "> Without ninja, compiling can take a very long time (2h) since it does not use multiple CPU cores. With ninja compiling takes 3-5 minutes on a 64-core machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install packaging ninja\n",
    "!ninja --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust MAX_JOBS to suit your machine. I used 8 for 64GB RAM \n",
    "%env MAX_JOBS=8\n",
    "\n",
    "%pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "Onto [Step 2: Title + Plot](./2_title_plot.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotomatic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
