{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Generate Samples\n",
    "\n",
    "First, we will take some books from the public domain and convert them into our representation of a Story object so that we have some solid references to build on.\n",
    "\n",
    "## Data Model\n",
    "\n",
    "In order to keep building up the context for the LLM to have more an more details as we get them, we will make a data model and put it in [model.py](./model.py) so that all of the notebooks can share it. We will build up a Story object and iterate over it to refine it, save it as a checkpoint to disk and then reload the Story in the next step. \n",
    "\n",
    "By using pydantic types, it will be easy to add other functionality like FastAPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U ipywidgets\n",
    "%pip install torch\n",
    "%pip install llama-index\n",
    "%pip install llama-index-llms-ollama\n",
    "# %pip install llama-index-llms-huggingface\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-embeddings-huggingface-api\n",
    "%pip install llama-index-embeddings-ollama\n",
    "# %pip install llama-index-extractors-entity\n",
    "\n",
    "%pip install tiktoken\n",
    "\n",
    "\n",
    "# %pip install --upgrade protobuf\n",
    "# %pip install -U bitsandbytes\n",
    "\n",
    "\n",
    "\n",
    "# %pip install llama-parse\n",
    "# # !pip install replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the use of asyncio in Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the LLM works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_text\n",
    "model_text.llm.complete(\"Hello, my name is Matt\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super verbose debugging\n",
    "import llama_index_log_handler\n",
    "import logging\n",
    "callback_manager = llama_index_log_handler.callback_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Some Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Story \n",
    "\n",
    "\"Gift of the Magi\"\n",
    "\n",
    "Let's start with a short one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the book\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "gift_of_the_magi_path = \"training/books/gift_of_the_magi.txt\"\n",
    "gift_of_the_magi_docs = SimpleDirectoryReader(input_files=[gift_of_the_magi_path], filename_as_id=True).load_data()\n",
    "gift_of_the_magi_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run extractors over all of the nodes of the story\n",
    "We've written a couple custom LlamaIndex extractors that use the LLM to parse out Character and Scene details. \n",
    "\n",
    "Check [llama_index_extractors.py](./llama_index_extractors.py) and [LlamaIndex Metadata Extraction Docs](https://docs.llamaindex.ai/en/stable/module_guides/indexing/metadata_extraction/) to see how they work.\n",
    "\n",
    "Let's extract all the characters, scenes and summary from the nodes in story.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_text\n",
    "from llama_index_extractors import CharacterExtractor, SceneExtractor\n",
    "\n",
    "from llama_index.core.extractors import TitleExtractor, SummaryExtractor\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "# TODO: Vector store\n",
    "\n",
    "llama_index_log_handler.enable()\n",
    "# llama_index_log_handler.disable()\n",
    "\n",
    "# Split chunks of text by sentences\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=1024, \n",
    "    chunk_overlap=128,\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "\n",
    "# Our custom CharacterExtractor\n",
    "character_extractor = CharacterExtractor(\n",
    "    llm=model_text.llm, \n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "\n",
    "# Our custom SceneExtractor\n",
    "scene_extractor = SceneExtractor(\n",
    "    llm=model_text.llm, \n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "\n",
    "# Generic TitleExtractor\n",
    "title_extractor = TitleExtractor(\n",
    "    nodes=5, \n",
    "    llm=model_text.llm, \n",
    "    callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "# Generic SumamryExtractor\n",
    "summary_extractor = SummaryExtractor(\n",
    "    nodes=5, \n",
    "    llm=model_text.llm, \n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "\n",
    "# Setup all the extractors in the pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        sentence_splitter,\n",
    "        title_extractor, \n",
    "        # qa_extractor,\n",
    "        summary_extractor,\n",
    "        scene_extractor,\n",
    "        character_extractor,\n",
    "        model_text.embedding,\n",
    "        # entity_extractor,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "magi_nodes = pipeline.run(\n",
    "    documents=gift_of_the_magi_docs,\n",
    "    in_place=True,\n",
    "    show_progress=True,\n",
    "    callback_manager=callback_manager,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the first node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "node = magi_nodes[0]\n",
    "characters = node.metadata['characters']\n",
    "characters_md = \"## Characters\\n\\n\" + characters.replace('\\n', '\\n\\n')\n",
    "display(Markdown(characters_md))\n",
    "\n",
    "# scenes = node.metadata['scenes']\n",
    "# scenes_md = \"## Scenes\\n\\n\" + scenes.replace('\\n', '\\n\\n')\n",
    "# display(Markdown(scenes_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = node.metadata['characters']\n",
    "characters_md = \"## Characters\\n\\n\" + characters.replace('\\n', '\\n\\n')\n",
    "display(Markdown(characters_md))\n",
    "\n",
    "# scenes = node.metadata['scenes']\n",
    "# scenes_md = \"## Scenes\\n\\n\" + scenes.replace('\\n', '\\n\\n')\n",
    "# display(Markdown(scenes_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize Nodes into a Story\n",
    "\n",
    "Let's summarize all of the data gathered by the nodes.\n",
    "\n",
    "[LlamaIndex Query Docs](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/)\n",
    "\n",
    "For the different modes refer to [LlamaIndex Response Synthesizer docs](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/#configuring-the-response-mode)\n",
    "\n",
    "**tree_summarize**: Query the LLM using the summary_template prompt as many times as needed so that all concatenated chunks have been queried, resulting in as many answers that are themselves recursively used as chunks in a tree_summarize LLM call and so on, until there's only one chunk left, and thus only one final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_text\n",
    "\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "from llama_index.core.prompts.prompt_type import PromptType\n",
    "\n",
    "llama_index_log_handler.enable()\n",
    "# llama_index_log_handler.disable()\n",
    "\n",
    "# tree_summarize_prompt = (\n",
    "#     \"Context information from multiple sources is below.\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"Given the information from multiple sources and not prior knowledge, \"\n",
    "#     \"answer the query.\\n\"\n",
    "#     \"Query: {query_str}\\n\"\n",
    "#     \"Answer: \"\n",
    "# )\n",
    "# tree_summarize_prompt_tpl = PromptTemplate(tree_summarize_prompt, prompt_type=PromptType.SUMMARY)\n",
    "tree_summarize_prompt = (\n",
    "    \"Here are multiple Story objects that were generated from different parts of the same story.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources about the same story and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "tree_summarize_prompt_tpl = PromptTemplate(tree_summarize_prompt, prompt_type=PromptType.SUMMARY)\n",
    "\n",
    "# refine_prompt = (\n",
    "#     \"The original query is as follows: {query_str}\\n\"\n",
    "#     \"We have provided an existing answer: {existing_answer}\\n\"\n",
    "#     \"We have the opportunity to refine the existing answer \"\n",
    "#     \"(only if needed) with some more context below.\\n\"\n",
    "#     \"------------\\n\"\n",
    "#     \"{context_msg}\\n\"\n",
    "#     \"------------\\n\"\n",
    "#     \"Given the new context, refine the original answer to better \"\n",
    "#     \"answer the query. \"\n",
    "#     \"If the context isn't useful, return the original answer.\\n\"\n",
    "#     \"Refined Answer: \"\n",
    "# )\n",
    "# tree_summarize_prompt_tpl = PromptTemplate(refine_prompt, prompt_type=PromptType.REFINE)\n",
    "\n",
    "# Loop over all the nodes and generate a combined response\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    # structured_answer_filtering=False, # Improve the quality of the structured answer, but needs tool calling\n",
    "    response_mode=ResponseMode.TREE_SUMMARIZE, \n",
    "    # response_mode=ResponseMode.REFINE,\n",
    "    use_async=True, \n",
    "    llm=model_text.llm,\n",
    "    callback_manager=llama_index_log_handler.callback_manager,\n",
    "    # summary_template=tree_summarize_prompt_tpl,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "response_str = response_synthesizer.get_response(\n",
    "    \"Combine all JSON story objects into one single collective JSON object with the same format\", \n",
    "    [f\"Summary:\\n\\n{node.metadata['section_summary']}\\n\\n\\nCharacters:\\n\\n{node.metadata['characters']}\\n\\n\\nScenes:\\n\\n{node.metadata['scenes']}\" for node in magi_nodes],\n",
    ")\n",
    "response_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "index = VectorStoreIndex(nodes=magi_nodes, embed_model=model_text.embedding)\n",
    "query_engine = index.as_query_engine(llm=model_text.llm)\n",
    "response = query_engine.query(\"Combine all JSON story objects into one single collective JSON object with the same format\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Extraction\n",
    "\n",
    "Sometimes for lower parameter models, we won't get JSON back that we need. One way to get around that is to retry with increasing temperature (randomness) until we have one that looks good. [story_extractor.py](./story_extractor.py) has a funtion `extract_story_json` that does this for us. There are other libraries like `Guidance` that will do this, but they won't do any sort of retry on models they don't support. The `llama-index` pydantic extraction has the same problem. We will roll our own to make it flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from story_extractor import extract_story_json\n",
    "\n",
    "story_json = extract_story_json(model_text.llm, response_str)\n",
    "story_json.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.data_structs import Node\n",
    "from llama_index.core.schema import NodeWithScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import DocumentSummaryIndex\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "print(type(magi_nodes))\n",
    "print(magi_nodes)\n",
    "\n",
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    gift_of_the_magi_docs,\n",
    "    llm=model_text.llm,\n",
    "    transformations=[\n",
    "        sentence_splitter,\n",
    "        title_extractor, \n",
    "        # qa_extractor,\n",
    "        summary_extractor,\n",
    "        scene_extractor,\n",
    "        character_extractor,\n",
    "        model_text.embedding,\n",
    "        # entity_extractor,\n",
    "    ],\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    "    embed_model=model_text.embedding,\n",
    "    embed_summaries=True,\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "\n",
    "doc_summary_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = doc_summary_index.get_document_summary(gift_of_the_magi_path)\n",
    "\n",
    "\n",
    "# \"combine all Story JSON objects into one single collective JSON object with the same format\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Markdown, display\n",
    "\n",
    "# # Summarize the book\n",
    "# plot_summary = doc_summary_index.get_document_summary(DOC_ID)\n",
    "\n",
    "# # Display the summary as markdown\n",
    "# display(Markdown(f\"## Plot Summary\\n\\n{plot_summary}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotomatic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
